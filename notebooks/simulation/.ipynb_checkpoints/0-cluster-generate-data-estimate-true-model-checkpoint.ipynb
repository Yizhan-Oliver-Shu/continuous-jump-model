{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b82d6ed-1bca-46eb-81e3-91c70bf7c5fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, os\n",
    "from os.path import expanduser\n",
    "## actions required!!!!!!!!!!!!!!!!!!!! change your folder path \n",
    "path = \"~/Documents/G3_2/regime-identification\"\n",
    "path = expanduser(path)\n",
    "sys.path.append(path)\n",
    "\n",
    "path_file = \"/scratch/network/yizhans/G3_2/simulation\"\n",
    "path_data = f\"{path_file}/data\"\n",
    "path_estimation = f\"{path_file}/estimation\"\n",
    "path_score = f\"{path_file}/score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46d65ded-9cd0-429b-9568-e7b0d5fb6746",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from numpy.random import RandomState\n",
    "random_state = RandomState(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d7f863c-1c14-48bc-be92-13c96d4302cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from regime.simulation_helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a70b13-94fc-4b44-a66e-bac479db887f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 0-generate-data-estimate-true-model\n",
    "\n",
    "In this notebook we systematically generate the simulation data, estimate the labels and probability by the true HMM model, and score them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cd5a64-8018-4c23-8165-a14d2f5be227",
   "metadata": {},
   "source": [
    "# Original Jump paper\n",
    "## Vanilla: 9 combinations\n",
    "\n",
    "- scale: We use the parameters estimated in the classical Hardy's paper, and convert into three scales: **daily, weekly, monthly**, with decreasing persistency.\n",
    "- length: We simulate seqs of different length: 250, 500, 1000.\n",
    "\n",
    "For each combo, we simulate `n_t=1000` seqs. The data in each combo are saved in a batch, thus in the shape of `(n_t, n_s, n_f)`. Also since we need to do feature engineering, every seq is 20 periods longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a5fcf73-d489-4219-be4d-79496bd5a60d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scale_lst = [\"daily\", \"weekly\", \"monthly\"]\n",
    "n_s_lst = [250, 500, 1000]\n",
    "key_data_list = [f\"{scale}_{n_s}\" for scale in scale_lst for n_s in n_s_lst]\n",
    "\n",
    "n_buffer, n_t, n_c = 20, 1024, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92386364-06af-4029-b92a-af74830fb994",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 317/1000 [00:06<00:13, 50.65it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m key_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscale\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_s\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# simulate X_raw, Z.\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m Xs, Zs \u001b[38;5;241m=\u001b[39m \u001b[43msample_from_hmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhmm_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_s\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mn_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m Zs \u001b[38;5;241m=\u001b[39m Zs[:, \u001b[38;5;241m-\u001b[39mn_s:]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# save raw data\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/G3_2/regime-identification/regime/simulation_helper.py:51\u001b[0m, in \u001b[0;36msample_from_hmm\u001b[0;34m(hmm_model, n_trials, n_samples, random_state)\u001b[0m\n\u001b[1;32m     49\u001b[0m     X, Z \u001b[38;5;241m=\u001b[39m hmm_model\u001b[38;5;241m.\u001b[39msample(n_samples\u001b[38;5;241m=\u001b[39mn_samples, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m     50\u001b[0m     Xs[i] \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m---> 51\u001b[0m     Zs[i] \u001b[38;5;241m=\u001b[39m Z    \n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Xs, Zs\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for scale in scale_lst:\n",
    "    # get a true HMM model\n",
    "    hmm_true = get_GaussianHMM_model(*load_hardy_params(scale), random_state=random_state)\n",
    "    for n_s in n_s_lst:\n",
    "        # generate key for data\n",
    "        key_data = f\"{scale}_{n_s}\"\n",
    "        # simulate X_raw, Z.\n",
    "        Xs, Zs = sample_from_hmm(hmm_true, n_trials=n_t, n_samples=n_s+n_buffer, random_state=random_state)\n",
    "        Zs = Zs[:, -n_s:]\n",
    "        # save raw data\n",
    "        np_save_print(f\"{path_data}/X_raw_{key_data}.npy\", Xs, \"X raw\")\n",
    "        np_save_print(f\"{path_data}/Z_{key_data}.npy\", Zs, \"Z\")        \n",
    "        # estimate by the true HMM model.\n",
    "        labels_arr, proba_arr = HMM_estimate_result(hmm_true, Xs[:, -n_s:])\n",
    "        # save estimation results\n",
    "        np_save_print(f\"{path_estimation}/labels_{key_data}_true.npy\", labels_arr, \"labels\")\n",
    "        np_save_print(f\"{path_estimation}/proba_{key_data}_true.npy\", proba_arr, \"proba\")\n",
    "        # score the estimation by the true model.\n",
    "        acc_arr = scorer_batch(accuracy_each_cluster, Zs, labels_arr, )\n",
    "        idx = get_idx_have_all_clusters(Zs, n_c)\n",
    "        roc_auc_arr = scorer_batch(roc_auc_score, Zs, proba_arr[:, :, 1], idx_subset=idx)\n",
    "        # save scores\n",
    "        np_save_print(f\"{path_score}/acc_{key_data}_true\", acc_arr, \"accuracy score\")\n",
    "        np_save_print(f\"{path_score}/roc_auc_{key_data}_true\", roc_auc_arr, \"roc auc score\")\n",
    "        # print for sanity check\n",
    "        print(f\"{key_data} data. BAC: {np.nanmean(acc_arr, 0).mean()}, roc_auc: {np.nanmean(roc_auc_arr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ed3f3d-7b23-4455-94a3-8762c542ac2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
