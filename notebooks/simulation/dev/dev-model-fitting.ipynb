{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49b52406-f768-4d6a-a6e3-ed4f387f1a2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, os\n",
    "from os.path import expanduser\n",
    "## actions required!!!!!!!!!!!!!!!!!!!! change your folder path \n",
    "path = expanduser(\"~/Documents/G3_2/regime-identification\")\n",
    "sys.path.append(path)\n",
    "\n",
    "path_file = expanduser(\"~/data/G3_2/regime-identification/simulation\")\n",
    "path_data = f\"{path_file}/data\"\n",
    "path_estimation = f\"{path_file}/estimation\"\n",
    "path_score = f\"{path_file}/score\"\n",
    "path_figure = f\"{path_file}/figure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98366a90-b577-4585-9741-c2ef6f9b5c60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import trange, tqdm\n",
    "from numpy.random import RandomState\n",
    "random_state = RandomState(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ecb5e0e-544f-4067-94f5-95e1723ebc65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from regime.jump import *\n",
    "from regime.simulation_helper import *\n",
    "from regime.stats import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33973c44-f403-40ea-9e11-0b8b28f4be33",
   "metadata": {},
   "source": [
    "# Model fitting dev\n",
    "\n",
    "In this nb we develop the functionality of training one model on a batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bd9c38f-341e-402c-9d02-8999225bca10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # true model\n",
    "# def model_true_fit_batch(model, Xs):\n",
    "#     \"\"\"\n",
    "#     fit the true model on a batch of data. save model parameters, and fitted labels & proba\n",
    "#     There is no need to estimate the model on the data; only call viterbi and forward-backward algos.\n",
    "#     model parameters are copies of the true params.\n",
    "        \n",
    "#     Paramaters:\n",
    "#     ---------------------\n",
    "#     model: a model instance.\n",
    "    \n",
    "#     Xs: array of size (n_t, n_s, n_f)\n",
    "#         input data\n",
    "        \n",
    "#     Returns:\n",
    "#     -------------------------\n",
    "#     model_params_arr: (n_t, n_c**2 + n_c)\n",
    "    \n",
    "#     labels_arr: array of size (n_t, n_s)\n",
    "    \n",
    "#     proba_arr: array of size (n_t, n_s, n_c)\n",
    "#     \"\"\"\n",
    "#     n_t, n_s, _ = Xs.shape\n",
    "#     n_c = model.n_components\n",
    "#     # model parameters, true values\n",
    "#     model_params = combine_model_param_estimation(model.means_.squeeze(), model.covars_.squeeze(), model.transmat_)\n",
    "#     model_params_arr = np.repeat(model_params[np.newaxis, ...], n_t, 0)\n",
    "#     # fitted labels & proba\n",
    "#     labels_arr = np.empty((n_t, n_s), dtype=np.int32)\n",
    "#     proba_arr = np.empty((n_t, n_s, n_c))\n",
    "    \n",
    "#     # for i_trial in tqdm(range(n_t)):\n",
    "#     for i_trial in range(n_t):   # really fast, no need to tqdm, unless the state space becomes large.\n",
    "#         X = Xs[i_trial]\n",
    "#         labels_arr[i_trial] = model.predict(X)\n",
    "#         proba_arr[i_trial] = model.predict_proba(X)\n",
    "#     return model_params_arr, labels_arr, proba_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4aba1df0-a42c-488d-8ebd-d2ac61ea39e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# our models\n",
    "def model_fit_batch(model, Xs, Zs):\n",
    "    \"\"\"\n",
    "    fit a model on a batch of data. save model parameters, and fitted labels & proba.\n",
    "    need the true labels to do alignments (i.e. the permutation with the highest overall accuracy)\n",
    "\n",
    "        \n",
    "    Paramaters:\n",
    "    ---------------------\n",
    "    model: a model instance.\n",
    "    \n",
    "    Xs: array of size (n_t, n_s, n_f)\n",
    "        input data\n",
    "        \n",
    "    Zs: array (n_t, n_s)\n",
    "        true labels\n",
    "        \n",
    "    Returns:\n",
    "    -------------------------\n",
    "    model_params_arr: (n_t, n_c**2 + n_c)\n",
    "    \n",
    "    labels_arr: array of size (n_t, n_s)\n",
    "    \n",
    "    proba_arr: array of size (n_t, n_s, n_c)\n",
    "    \"\"\"\n",
    "    n_t, n_s, _ = Xs.shape\n",
    "    n_c = model.n_components\n",
    "\n",
    "    res_list = []\n",
    "    # estimate\n",
    "    # for i_t in trange(n_t):\n",
    "    for i_t in range(n_t):\n",
    "        X, Z = Xs[i_t], Zs[i_t]\n",
    "        # fit\n",
    "        model.fit(X)\n",
    "        # save dict result\n",
    "        res_list.append(extract_results_from_model(model, X_=X[:, 0]))\n",
    "    # dict of results\n",
    "    res = combine_list_dict(res_list)\n",
    "    # align with true labels\n",
    "    res = align_estimation_results_batch(Zs, res)\n",
    "    # combine model params\n",
    "    model_params_arr = combine_model_param_estimation(res[\"means_\"], res[\"covars_\"], res[\"transmat_\"])\n",
    "    labels_arr = res[\"labels_\"]; proba_arr = res[\"proba_\"]\n",
    "    return model_params_arr, labels_arr, proba_arr\n",
    "    \n",
    "# helpers\n",
    "def weighted_mean_vol(X, proba_):\n",
    "    n_c = proba_.shape[1]\n",
    "    means_, covars_ = np.full(n_c, np.nan), np.full(n_c, np.nan)\n",
    "    total_weight = proba_.sum(0)\n",
    "    idx = (total_weight>0)\n",
    "    weighted_sum = X @ proba_\n",
    "    means_[idx] = weighted_sum[idx] / total_weight[idx]\n",
    "    weighted_sum_square = ((X[:, np.newaxis] - means_[np.newaxis, :])**2 * proba_).sum(0)\n",
    "    covars_[idx] = weighted_sum_square[idx] / total_weight[idx]\n",
    "    return means_, covars_\n",
    "\n",
    "def raise_labels_to_proba_(labels_, n_c):\n",
    "    \"\"\"\n",
    "    raise one labels_ into a proba_\n",
    "    labels_: (n_s,)\n",
    "    \"\"\"\n",
    "    n_s = len(labels_)\n",
    "    proba_ = np.zeros((n_s, n_c))\n",
    "    np.put_along_axis(proba_, indices=labels_[..., np.newaxis], values=1., axis=-1)\n",
    "    return proba_  \n",
    "\n",
    "def raise_labels_to_proba_batch(labels_arr, n_c):\n",
    "    \"\"\"\n",
    "    labels_arr: (n_t, n_s)\n",
    "    \"\"\"\n",
    "    n_t, n_s = labels_arr.shape\n",
    "    proba_arr = np.zeros((n_t, n_s, n_c))\n",
    "    np.put_along_axis(proba_arr, indices=labels_arr[..., np.newaxis], values=1., axis=-1)\n",
    "    return proba_arr    \n",
    "\n",
    "def extract_results_from_model(model, X_=None):\n",
    "    \"\"\"\n",
    "    extract the estimation results from one model.\n",
    "    The 1d sequence X is needed to compute the weighted means, covars.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------------------------------------\n",
    "    model:\n",
    "    \n",
    "    X_: array (n_s,). default None.\n",
    "    \n",
    "    Returns:\n",
    "    ---------------------------------------\n",
    "    result: dict\n",
    "    \"\"\"\n",
    "    n_c = model.n_components\n",
    "    result = {}\n",
    "    # proba\n",
    "    if hasattr(model, \"proba_\"):\n",
    "        result[\"proba_\"] = model.proba_\n",
    "    else:\n",
    "        result[\"proba_\"] = raise_labels_to_proba_(model.labels_, n_c)\n",
    "        \n",
    "    # label\n",
    "    if hasattr(model, \"labels_\"):\n",
    "        result[\"labels_\"] = model.labels_\n",
    "    else:\n",
    "        result[\"labels_\"] = model.proba_.argmax(axis=-1).astype(np.int32)\n",
    "        \n",
    "    # means covars\n",
    "    if hasattr(model, \"means_\"):\n",
    "        result[\"means_\"] = model.means_\n",
    "        result[\"covars_\"] = model.covars_\n",
    "    else:\n",
    "        # compute weighted average by proba_\n",
    "        result[\"means_\"], result[\"covars_\"] = weighted_mean_vol(X_, result[\"proba_\"])\n",
    "        \n",
    "    # transmat\n",
    "    if hasattr(model, \"transmat_\"):\n",
    "        result[\"transmat_\"] = model.transmat_\n",
    "    else:\n",
    "        # empirical\n",
    "        result[\"transmat_\"] = empirical_trans_mx(result[\"labels_\"])\n",
    "    return result\n",
    "\n",
    "def combine_list_dict(dict_list):\n",
    "    \"\"\"\n",
    "    input is a list of dictionaries, all with the same keys.\n",
    "    return a dict with the same keys, value is the stacked array.\n",
    "    \"\"\"\n",
    "    keys = dict_list[0].keys()\n",
    "    res = {key: np.array([dict_[key] for dict_ in dict_list]) for key in keys}\n",
    "    return res  \n",
    "\n",
    "def align_estimation_results_batch(Zs_true, res):\n",
    "    \"\"\"\n",
    "    align a batch of estimation results with the true labels, i.e. find the optimal permutation for each sample.\n",
    "    results include labels_, proba_, means_, covars_, transmat_.\n",
    "    \"\"\"\n",
    "    n_c, n_t = len(np.unique(Zs_true)), len(Zs_true)\n",
    "    # all the perms\n",
    "    all_perms = generate_all_perms_as_arr(n_c) \n",
    "    # all the possible perms of labels\n",
    "    labels_all_perms = permute_labels(res[\"labels_\"], all_perms)\n",
    "    # score accuracy for each perm\n",
    "    acc_all_perms = scorer_batch(accuracy_score, Zs_true, labels_all_perms, has_params=True) # of shape (n_t, n_p)\n",
    "    # best perm for each trial \n",
    "    best_perm_idx = acc_all_perms.argmax(-1) # shape (n_t,)\n",
    "    best_perm = all_perms[best_perm_idx] # (n_t, n_c)\n",
    "    # take the corresponding perm for labels\n",
    "    res[\"labels_\"] = np.take_along_axis(labels_all_perms, best_perm_idx[:, np.newaxis, np.newaxis], axis=-1).squeeze(axis=-1)\n",
    "    res[\"proba_\"] = np.take_along_axis(res[\"proba_\"], best_perm[:, np.newaxis, :], -1)\n",
    "    res[\"means_\"] = np.take_along_axis(res[\"means_\"], best_perm, -1)\n",
    "    res[\"covars_\"] = np.take_along_axis(res[\"covars_\"], best_perm, -1)\n",
    "    res[\"transmat_\"] = np.take_along_axis(res[\"transmat_\"][np.arange(n_t)[:, np.newaxis], best_perm], best_perm[:, np.newaxis, :], -1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "184f542f-ad6d-48c6-8072-2d9554ac4255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_fit_batch_with_params(model, Xs, Zs, param_grid=None):\n",
    "    \"\"\"\n",
    "    fit a model on a batch of data. save model parameters, and fitted labels & proba.\n",
    "    the model can have a param_grid for hyperparam tuning.\n",
    "    need the true labels to do alignments (i.e. the permutation with the highest overall accuracy)\n",
    "\n",
    "        \n",
    "    Paramaters:\n",
    "    ---------------------\n",
    "    model: a model instance.\n",
    "    \n",
    "    Xs: array of size (n_t, n_s, n_f)\n",
    "        input data\n",
    "        \n",
    "    Zs: array (n_t, n_s)\n",
    "        true labels\n",
    "        \n",
    "    param_grid: dict, default None\n",
    "        if None, will call `model_fit_batch` directly.\n",
    "        \n",
    "    Returns:\n",
    "    -------------------------\n",
    "    model_params_arr: (n_t, n_c**2 + n_c, n_l)\n",
    "    \n",
    "    labels_arr: array of size (n_t, n_s, n_l)\n",
    "    \n",
    "    proba_arr: array of size (n_t, n_s, n_c, n_l)\n",
    "    \"\"\"\n",
    "    if param_grid is None: # no hyperparams\n",
    "        return model_fit_batch(model, Xs, Zs)\n",
    "    \n",
    "    PG = ParameterGrid(param_grid)\n",
    "    model_params_arr_list, labels_arr_list, proba_arr_list = [], [], []\n",
    "    for param_ in tqdm(PG):\n",
    "        model.set_params(**param_)\n",
    "        model_params_arr, labels_arr, proba_arr = model_fit_batch(model, Xs, Zs)\n",
    "        model_params_arr_list.append(model_params_arr)\n",
    "        labels_arr_list.append(labels_arr)\n",
    "        proba_arr_list.append(proba_arr)\n",
    "    return np.stack(model_params_arr_list, axis=-1), np.stack(labels_arr_list, axis=-1), np.stack(proba_arr_list, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7705f5-d121-453b-b490-11a0eb05210c",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b6e411a-8983-4dbf-aba8-1774b4040b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_t=10\n",
    "Xs_zheng = np.load(f\"{path_data}/Xs_daily_1000_zheng.npy\")[:n_t]\n",
    "Xs_HMM = np.load(f\"{path_data}/Xs_daily_1000_HMM.npy\")[:n_t]\n",
    "Zs = np.load(f\"{path_data}/Zs_daily_1000.npy\")[:n_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53ef4303-2264-4d48-891d-143ffae8f711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_c=2\n",
    "model_hmm = GaussianHMM_model(n_c, random_state=random_state)\n",
    "model_discrete = jump_model(n_c, state_type=\"discrete\", jump_penalty=1e2, random_state=random_state)\n",
    "model_cont_mode = jump_model(n_c, state_type=\"cont\", grid_size=.02, mode_loss=True, jump_penalty=1e3, random_state=random_state)\n",
    "model_cont_no_mode = jump_model(n_c, state_type=\"cont\", grid_size=.02, mode_loss=False, jump_penalty=1e3, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a15484de-9dad-4690-a7d6-b345f8249546",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:04<00:00,  2.41it/s]\n"
     ]
    }
   ],
   "source": [
    "model_params_arr1, labels_arr1, proba_arr1 = model_fit_batch(model_hmm, Xs_HMM, Zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f9e5751-48fd-4846-8542-710940b6bd2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 147.82it/s]\n"
     ]
    }
   ],
   "source": [
    "model_params_arr2, labels_arr2, proba_arr2 = model_fit_batch(model_discrete, Xs_zheng, Zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e515fbca-11a3-439b-9c30-41d9a47ddb49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  7.73it/s]\n"
     ]
    }
   ],
   "source": [
    "model_params_arr3, labels_arr3, proba_arr3 = model_fit_batch(model_cont_mode, Xs_zheng, Zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91a3cfda-6d22-4f0a-b677-01d0b55f99e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  5.38it/s]\n"
     ]
    }
   ],
   "source": [
    "model_params_arr4, labels_arr4, proba_arr4 = model_fit_batch(model_cont_no_mode, Xs_zheng, Zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0a286fd-0cb4-44b2-954b-86a21aaad5ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid = {\"jump_penalty\": [1e2, 1e3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab7acd84-d659-4ee0-a060-52c74f5b2ff4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.16s/it]\n"
     ]
    }
   ],
   "source": [
    "model_params_arr5, labels_arr5, proba_arr5 = model_fit_batch_with_params(model_cont_no_mode, Xs_zheng, Zs, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c3b7b6-f1c6-4119-bf5b-eb559e44958c",
   "metadata": {},
   "source": [
    "# fitting on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e211dc-daa4-4a3c-b125-baba3bca803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit_many_datas_models(key_data_list, key_feat_list, model_dict, param_grid, path_data, path_estimation, start = 0, end = -1, sub_job_no = \"\"):\n",
    "    \"\"\"\n",
    "    train a collection of models, w/ hyperparams to tune, on a batch of data from many datasets.\n",
    "    \"\"\"\n",
    "    N_combos = len(key_data_list) * len(key_feat_list) * len(model_dict)\n",
    "    count=0\n",
    "    time_old = time.time(); total_time=0.\n",
    "    for key_data, key_feat in product(key_data_list, key_feat_list):\n",
    "        # load data\n",
    "        Xs = np.load(f\"{path_data}/Xs_{key_data}_{key_feat}.npy\")[start:end]\n",
    "        Zs = np.load(f\"{path_data}/Zs_{key_data}.npy\")[start:end]\n",
    "        for key_model, model in model_dict.items():\n",
    "            # train the model, on a param grid, on a batch of data\n",
    "            model_params_arr, labels_arr, proba_arr = model_fit_batch_with_params(model, Xs, Zs, param_grid)\n",
    "            # save results\n",
    "            save_estimation_results(model_params_arr, labels_arr, proba_arr, path_estimation, key_data, key_feat, key_model, sub_job_no)\n",
    "            time_now = time.time(); time_this_iter = time_now-time_old; total_time += time_this_iter; time_old = time_now\n",
    "            count+=1\n",
    "            print(f\"{count}/{N_combos} combos done. Time of this iter: {print_seconds(time_this_iter)}s. Total time: {print_seconds(total_time)}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8644d9-7e68-4109-9bab-3f9b0260d550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e5c5b1-7c9c-48ac-8464-4a3def5d5137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fe3581-31fe-4780-b4d0-e53e2013610d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7673af-0fe7-42f6-9c7b-8b264183ea4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c30f5055-b53b-4c3d-933d-0f4c20f57d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def temp(arr):\n",
    "    return pd.DataFrame(arr).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "382c6cf1-1f29-4f6e-a8cd-12160efa665c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.000789\n",
       "1    0.001830\n",
       "2    0.007772\n",
       "3    0.014325\n",
       "4    0.004589\n",
       "5    0.126405\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp(model_params_arr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b875bcf1-70c7-4f00-b4c8-451ab94b274e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.000723\n",
       "1    0.000868\n",
       "2    0.007905\n",
       "3    0.016297\n",
       "4    0.002576\n",
       "5    0.014046\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp(model_params_arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f3a230-8095-4ec7-aec8-368bca3658f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.000722\n",
       "1    0.000545\n",
       "2    0.008031\n",
       "3    0.014882\n",
       "4    0.002833\n",
       "5    0.013157\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp(model_params_arr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca127d2e-3425-4f9f-b74a-55fb1e73e557",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.000729\n",
       "1    0.000568\n",
       "2    0.008035\n",
       "3    0.014866\n",
       "4    0.003206\n",
       "5    0.014553\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp(model_params_arr4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5723fa4-0a9a-4f09-b9ad-ee4e0835317e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91585d32-100c-4dc3-8d96-db120250dae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0962438a-8034-420f-88c7-8fc700cc192e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1357f32b-8a0e-4362-a830-c7052aea22c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fba6fd-b038-46bd-9220-66e873826d74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4322c127-88b8-4b61-8569-f9ec1cf35513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
